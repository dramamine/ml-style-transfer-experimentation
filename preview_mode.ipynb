{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preview_mode.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dramamine/ml-style-transfer-experimentation/blob/main/preview_mode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpWwQOT8rzG"
      },
      "source": [
        "## Stylize Your Images In 5 Minutes\n",
        "\n",
        "Demo developed by [Sayak Paul](https://twitter.com/RisingSayak) and modified by [Marten Silbiger](https://metal-heart.org)\n",
        "\n",
        "and adamp\n",
        "\n",
        "Neural style transfer is one of the most interesting applications of deep learning. We've created a demo to help you recreate your images in the style of famous artists.\n",
        "\n",
        "![](https://storage.googleapis.com/download.tensorflow.org/models/tflite/arbitrary_style_transfer/table.png)\n",
        "\n",
        "The image and code id adapted from [this tutorial](https://www.tensorflow.org/lite/models/style_transfer/overview). \n",
        "\n",
        "## To start the demo click the play button to the left of each cell, one by one.\n",
        "\n",
        "You need to run the following cells in order to get stylized images.\n",
        "\n",
        "The easiest way to do that is to click on the cell and press Shift + Enter or press the play button. Each cell takes between 10-30 seconds to execute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN-X3Rna_euj"
      },
      "source": [
        "# Create an HD Render"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciYyUYVZT4IA"
      },
      "source": [
        "#@title 1. Setup üß∞\n",
        "#@markdown Just run this cell as is. ***Don't modify the code block.*** The setup should not take more than two minutes. \n",
        "print(\"Installing tf-nightly...\")\n",
        "!pip uninstall -q -y tensorflow \n",
        "!pip install -q tf-nightly\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        " \n",
        "clear_output()\n",
        "\n",
        "print(\"tf-nightly installed.\")\n",
        "\n",
        "print(\"Mounting google drive...\")\n",
        "#@title 2. Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "# if you need to see inside or check the directory structure, you can use this\n",
        "#!apt-get install tree\n",
        "#!tree '/content/drive/MyDrive/images/'\n",
        "print(\"google drive mounted.\")\n",
        "\n",
        "# Download the style bottleneck and transfer networks\n",
        "print('Downloading the model files...')\n",
        "\n",
        "style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/predict/1?lite-format=tflite')\n",
        "style_transform_path = style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/transfer/1?lite-format=tflite')\n",
        "\n",
        "print('Model files downloaded...')\n",
        "\n",
        "# other deps\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "import numpy as np\n",
        "import io\n",
        "import os\n",
        "\n",
        "print('You are all set!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5FEZUBxlBq-"
      },
      "source": [
        "#@title 2. Configure üçπ\n",
        "from PIL import Image\n",
        "\n",
        "google_drive_base = '/content/drive/MyDrive/images/'\n",
        "content_image_path = 'prettyearth/1568.jpg' #@param\n",
        "style_image_path = 'alex-grey/wp2705517-alex-grey-backgrounds-desktop.jpg' #@param\n",
        "\n",
        "#@title 3b. Divide our style image and content image into sub-blocks.\n",
        "#@markdown Here you need to set the number of rows and columns for subdivisions.\n",
        "#@markdown Each region should be 256x256 or greater, ex. 1920x1080 should be\n",
        "#@markdown _at most_ 7 columns by 4 rows. Each cell was taking me about 90 seconds to generate.\n",
        "\n",
        "cols = 3 #@param {type:\"slider\", min:1, max:7, step:1}\n",
        "rows = 2 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "\n",
        "#@markdown Should we calculate one style for the entire style image? Or should we \n",
        "#@markdown divide the style image into tiles just like we do with the the content?\n",
        "use_tiled_style_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Edge size in final image. A corner piece will have (384-edge_size)^2\n",
        "#@markdown of non-discarded content, while a center piece will be (384-2*edge_size)^2.\n",
        "edge_size = 8 #@param {type:\"slider\", min:0, max:32, step:1}\n",
        "\n",
        "content_image = Image.open(google_drive_base+content_image_path)\n",
        "style_image = Image.open(google_drive_base+style_image_path)\n",
        "\n",
        "print('files loaded.')\n",
        "\n",
        "import math\n",
        "def imshow(image, title=None):\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "def get_array_of_pieces(title, img, cols, rows, show=False):\n",
        "  pieces = []\n",
        "  print(title,\"image size:\", img.size)\n",
        "  w = math.floor(img.size[0]/cols)\n",
        "  h = math.floor(img.size[1]/rows)\n",
        "  print(\"using cell size:\", w, \"x\", h)\n",
        "\n",
        "  if (w < 256 or h < 256):\n",
        "    print(\"WARNING: That size seems a bit small and will probably result in stretching.\")\n",
        "\n",
        "  # scale edge_size\n",
        "  we = w*edge_size/384\n",
        "  he = h*edge_size/384\n",
        "\n",
        "  for r in range(0, rows):\n",
        "    for c in range(0, cols):\n",
        "      el = 0 if c == 0 else we\n",
        "      er = 0 if c == cols-1 else we\n",
        "      eu = 0 if r == 0 else he\n",
        "      ed = 0 if r == rows-1 else he\n",
        "\n",
        "      region = img.crop( (c*w-el, r*h-eu, (c+1)*w+er, (r+1)*h+ed) )\n",
        "      pieces.append(region)\n",
        "      if show:\n",
        "        plt.subplot(rows, cols, r*cols+c+1)\n",
        "        imshow(region)\n",
        "  return pieces\n",
        "\n",
        "# plt.figure(figsize=(10, 10))\n",
        "content_pieces = get_array_of_pieces(\"content\", content_image, cols, rows, True)\n",
        "if use_tiled_style_image:\n",
        "  style_pieces = get_array_of_pieces(\"style\", style_image, cols, rows)\n",
        "else: \n",
        "  style_pieces = list([style_image])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6ovDQ_pP2S_"
      },
      "source": [
        "#@title 3. Preprocess üëæ\n",
        "\n",
        "#@markdown ***Don't modify this code block.***\n",
        "\n",
        "# Function to load an image from a file, and add a batch dimension.\n",
        "def load_img(path_to_img):\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img\n",
        "\n",
        "# Function to load an image from a file, and add a batch dimension.\n",
        "def load_content_img(image_pixels):\n",
        "    if image_pixels.shape[-1] == 4:\n",
        "        image_pixels = Image.fromarray(image_pixels)\n",
        "        img = image_pixels.convert('RGB')\n",
        "        img = np.array(img)\n",
        "        img = tf.convert_to_tensor(img)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = img[tf.newaxis, :]\n",
        "        return img\n",
        "    elif image_pixels.shape[-1] == 3:\n",
        "        img = tf.convert_to_tensor(image_pixels)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = img[tf.newaxis, :]\n",
        "        return img\n",
        "    elif image_pixels.shape[-1] == 1:\n",
        "        raise Error('Grayscale images not supported! Please try with RGB or RGBA images.')\n",
        "    print('Exception not thrown')\n",
        "\n",
        "# Function to pre-process by resizing an central cropping it.\n",
        "def preprocess_image(image, target_dim):\n",
        "  # Resize the image so that the shorter dimension becomes 256px.\n",
        "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
        "  short_dim = min(shape)\n",
        "  scale = target_dim / short_dim\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "\n",
        "  # Central crop the image.\n",
        "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Function to run style prediction on preprocessed style image.\n",
        "def run_style_predict(preprocessed_style_image):\n",
        "  # Load the model.\n",
        "  interpreter = tf.lite.Interpreter(model_path=style_predict_path)\n",
        "\n",
        "  # Set model input.\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_style_image)\n",
        "\n",
        "  # Calculate style bottleneck.\n",
        "  interpreter.invoke()\n",
        "  style_bottleneck = interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"]\n",
        "      )()\n",
        "  # print('Style Bottleneck Shape:', style_bottleneck.shape)\n",
        "  return style_bottleneck\n",
        "\n",
        "def preprocessor(img, res):\n",
        "  img = load_content_img(np.array( img ))\n",
        "  return preprocess_image(img, res)\n",
        "\n",
        "print(\"Preprocessing content...\")\n",
        "tf_content_pieces = list(map(lambda x: \n",
        "    load_content_img(np.array(x)), content_pieces ))\n",
        "preprocessed_content_pieces = list(map(lambda x: \n",
        "    preprocessor(x, 384), content_pieces ))\n",
        "preprocessed_style_pieces = list(map(lambda x: \n",
        "    preprocessor(x, 256), style_pieces ))\n",
        "\n",
        "print(\"Calculating bottlenecks...this may take awhile.\")\n",
        "style_bottlenecks = list(map(lambda x: \n",
        "    run_style_predict(x), preprocessed_style_pieces ))\n",
        "style_bottleneck_contents = list(map(lambda x: \n",
        "    run_style_predict( preprocess_image(x, 256)), tf_content_pieces ))\n",
        "print(\"Content length:\", len(tf_content_pieces), \n",
        "      len(preprocessed_content_pieces), len(style_bottleneck_contents))\n",
        "print(\"Style length:\", len(preprocessed_style_pieces), \n",
        "      len(style_bottlenecks))\n",
        "\n",
        "print(\"Finished preprocessing.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22evvka6Rx-b"
      },
      "source": [
        "#@title 6. Stylize image ü•Å\n",
        "\n",
        "content_blending_ratio = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown You're encouraged to play with the different values of `content_blending_ratio`.\n",
        "\n",
        "# Run style transform on preprocessed style image\n",
        "def run_style_transform(style_bottleneck, preprocessed_content_image):\n",
        "  # Load the model.\n",
        "  interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
        "\n",
        "  # Set model input.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Set model inputs.\n",
        "  for index in range(len(input_details)):\n",
        "    if input_details[index][\"name\"]=='Conv/BiasAdd':\n",
        "      interpreter.set_tensor(input_details[index][\"index\"], style_bottleneck)\n",
        "    elif input_details[index][\"name\"]=='content_image':\n",
        "      interpreter.set_tensor(input_details[index][\"index\"], preprocessed_content_image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Transform content image.\n",
        "  stylized_image = interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"]\n",
        "      )()\n",
        "\n",
        "  return stylized_image\n",
        "\n",
        "def stylize(preprocessed_content_image, style_bottleneck, style_bottleneck_content):\n",
        "  # Blend the style bottleneck of style image and content image\n",
        "  style_bottleneck_blended = content_blending_ratio * style_bottleneck_content \\\n",
        "                            + (1 - content_blending_ratio) * style_bottleneck\n",
        "\n",
        "  # Stylize the content image using the style bottleneck.\n",
        "  stylized_image = run_style_transform(style_bottleneck_blended, preprocessed_content_image)\n",
        "\n",
        "  print('üêå', end=\"\")\n",
        "  return stylized_image\n",
        "\n",
        "print(\"Processing\", len(preprocessed_content_pieces), \"cells of content...\", end=\"\")\n",
        "\n",
        "if use_tiled_style_image:\n",
        "  stylized_pieces = list(map(lambda x,y,z: stylize(x,y,z), preprocessed_content_pieces, style_bottlenecks, style_bottleneck_contents ))\n",
        "else:\n",
        "  stylized_pieces = list(map(lambda x,z: stylize(x,style_bottlenecks[0],z), preprocessed_content_pieces, style_bottleneck_contents ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KA5Jy9XgTuO"
      },
      "source": [
        "#@title Sew it together and preview üîÆ\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# @TODO parameterize\n",
        "rows = 2\n",
        "cols = 3\n",
        "\n",
        "wh = 384\n",
        "\n",
        "im = Image.new('RGB', (\n",
        "    cols*wh-2*edge_size*(cols-1), \n",
        "    rows*wh-2*edge_size*(rows-1)\n",
        "))\n",
        "\n",
        "\n",
        "for r in range(0,rows):\n",
        "  for c in range(0,cols):\n",
        "    squeezed = tf.squeeze( stylized_pieces[c+r*cols] )\n",
        "    imaged = tf.keras.preprocessing.image.array_to_img(squeezed)\n",
        "\n",
        "    el = 0 if c == 0 else edge_size\n",
        "    er = 0 if c == cols-1 else edge_size\n",
        "    eu = 0 if r == 0 else edge_size\n",
        "    ed = 0 if r == rows-1 else edge_size\n",
        "\n",
        "\n",
        "    print(\"imaged size:\")\n",
        "    print(imaged.size)\n",
        "    cropped = imaged.crop((el,eu,wh-er,wh-ed))\n",
        "    print(\"cropped size:\")\n",
        "    print(cropped.size)\n",
        "\n",
        "    im.paste(cropped, (\n",
        "        c*wh - max(0, (2*c-1)*el), # left\n",
        "        r*wh - max(0, (2*r-1)*eu) # up\n",
        "    ))\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "imshow(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2T476eHzViZ"
      },
      "source": [
        "#@title Save your results üí•\n",
        "#@markdown If you experiment with the different `content_blending_ratio` values make sure you run this code block again in order to store your results online.\n",
        "import re\n",
        "\n",
        "def get_nice_name(path):\n",
        "  return re.search(r'.*/(.*).jpg', path).group(1)\n",
        "\n",
        "output = \"{0}output/{1}-{2}-hd-fusion-{3}x{4}-blend{5}-edge{6}{7}.jpg\".format(\n",
        "    google_drive_base,\n",
        "    get_nice_name(content_image_path),\n",
        "    get_nice_name(style_image_path),\n",
        "    im.size[0],\n",
        "    im.size[1],\n",
        "    int(10*content_blending_ratio),\n",
        "    edge_size,\n",
        "    \"-tiled\" if use_tiled_style_image else \"\"\n",
        ")\n",
        "\n",
        "im.save(output, \"JPEG\")\n",
        "print(\"Saved to:\", output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWqVyN4VZRxn"
      },
      "source": [
        "# Huge Refactor\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeQnyEePZgo6"
      },
      "source": [
        "#@title 1. Load Library Code\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "from PIL import Image\n",
        "from types import SimpleNamespace\n",
        "\n",
        "style_predict_path = tf.keras.utils.get_file(\n",
        "    'style_predict.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/predict/1?lite-format=tflite')\n",
        "style_transform_path = style_transform_path = tf.keras.utils.get_file(\n",
        "    'style_transform.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/transfer/1?lite-format=tflite')\n",
        "\n",
        "STYLE_SIZE = 256\n",
        "CONTENT_SIZE = 384\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "def get_array_of_pieces(img, cfg):\n",
        "  cols = cfg.cols\n",
        "  rows = cfg.rows\n",
        "  edge_size = cfg.edge_size\n",
        "\n",
        "  pieces = []\n",
        "  #print(title, \"image size:\", img.size)\n",
        "  w = math.floor(img.size[0]/cols)\n",
        "  h = math.floor(img.size[1]/rows)\n",
        "  #print(\"using cell size:\", w, \"x\", h)\n",
        "\n",
        "  if (w < STYLE_SIZE or h < STYLE_SIZE):\n",
        "    print(\"WARNING: That size seems a bit small and will probably result in stretching.\")\n",
        "\n",
        "  # scale edge_size\n",
        "  we = w*edge_size/CONTENT_SIZE\n",
        "  he = h*edge_size/CONTENT_SIZE\n",
        "\n",
        "  for r in range(0, rows):\n",
        "    for c in range(0, cols):\n",
        "      el = 0 if c == 0 else we\n",
        "      er = 0 if c == cols-1 else we\n",
        "      eu = 0 if r == 0 else he\n",
        "      ed = 0 if r == rows-1 else he\n",
        "\n",
        "      region = img.crop((c*w-el, r*h-eu, (c+1)*w+er, (r+1)*h+ed))\n",
        "      pieces.append(region)\n",
        "  return pieces\n",
        "\n",
        "def get_intermediate_tiles(img, cfg):\n",
        "  cols = cfg.cols\n",
        "  rows = cfg.rows\n",
        "  edge_size = cfg.edge_size\n",
        "\n",
        "  row_pieces = []\n",
        "  column_pieces = []\n",
        "  # print(title,\"image size:\", img.size)\n",
        "  w = math.floor(img.size[0]/cols)\n",
        "  h = math.floor(img.size[1]/rows)\n",
        "\n",
        "  # scale edge_size\n",
        "  we = w*edge_size/STYLE_SIZE\n",
        "  he = h*edge_size/STYLE_SIZE\n",
        "\n",
        "  for r in range(0, rows):\n",
        "    for c in range(0, cols-1):\n",
        "      el = 0 if c == 0 else we\n",
        "      er = 0 if c == cols-1 else we\n",
        "      eu = 0 if r == 0 else he\n",
        "      ed = 0 if r == rows-1 else he\n",
        "\n",
        "      region = img.crop(((c+0.5)*w-el, r*h-eu, (c+1.5)*w+er, (r+1)*h+ed))\n",
        "      row_pieces.append(region)\n",
        "\n",
        "  for r in range(0, rows-1):\n",
        "    for c in range(0, cols):\n",
        "      el = 0 if c == 0 else we\n",
        "      er = 0 if c == cols-1 else we\n",
        "      eu = 0 if r == 0 else he\n",
        "      ed = 0 if r == rows-1 else he\n",
        "\n",
        "      region = img.crop(((c)*w-el, (r+0.5)*h-eu, (c+1)*w+er, (r+1.5)*h+ed))\n",
        "      column_pieces.append(region)\n",
        "\n",
        "  return [row_pieces, column_pieces]\n",
        "\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.io.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  img = img[tf.newaxis, :]\n",
        "  return img\n",
        "\n",
        "\n",
        "\n",
        "# Function to load an image from a file, and add a batch dimension.\n",
        "def load_content_img(image_pixels):\n",
        "    if image_pixels.shape[-1] == 4:\n",
        "        image_pixels = Image.fromarray(image_pixels)\n",
        "        img = image_pixels.convert('RGB')\n",
        "        img = np.array(img)\n",
        "        img = tf.convert_to_tensor(img)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = img[tf.newaxis, :]\n",
        "        return img\n",
        "    elif image_pixels.shape[-1] == 3:\n",
        "        img = tf.convert_to_tensor(image_pixels)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        img = img[tf.newaxis, :]\n",
        "        return img\n",
        "    elif image_pixels.shape[-1] == 1:\n",
        "        raise Error(\n",
        "            'Grayscale images not supported! Please try with RGB or RGBA images.')\n",
        "    print('Exception not thrown')\n",
        "\n",
        "\n",
        "\n",
        "# Function to pre-process by resizing an central cropping it.\n",
        "def preprocess_image(image, target_dim):\n",
        "  # Resize the image so that the shorter dimension becomes 256px.\n",
        "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
        "  short_dim = min(shape)\n",
        "  scale = target_dim / short_dim\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "\n",
        "  # Central crop the image.\n",
        "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
        "\n",
        "  return image\n",
        "\n",
        "# Function to run style prediction on preprocessed style image.\n",
        "\n",
        "def run_style_predict(preprocessed_style_image):\n",
        "  # Load the model.\n",
        "  interpreter = tf.lite.Interpreter(model_path=style_predict_path)\n",
        "\n",
        "  # Set model input.\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_style_image)\n",
        "\n",
        "  # Calculate style bottleneck.\n",
        "  interpreter.invoke()\n",
        "  style_bottleneck = interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"]\n",
        "  )()\n",
        "  # print('Style Bottleneck Shape:', style_bottleneck.shape)\n",
        "  return style_bottleneck\n",
        "\n",
        "def preprocessor(img, res):\n",
        "  img = load_content_img(np.array(img))\n",
        "  return preprocess_image(img, res)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_style_transform(style_bottleneck, preprocessed_content_image):\n",
        "  # Load the model.\n",
        "  interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
        "\n",
        "  # Set model input.\n",
        "  input_details = interpreter.get_input_details()\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Set model inputs.\n",
        "  for index in range(len(input_details)):\n",
        "    if input_details[index][\"name\"] == 'Conv/BiasAdd':\n",
        "      interpreter.set_tensor(input_details[index][\"index\"], style_bottleneck)\n",
        "    elif input_details[index][\"name\"] == 'content_image':\n",
        "      interpreter.set_tensor(\n",
        "          input_details[index][\"index\"], preprocessed_content_image)\n",
        "  interpreter.invoke()\n",
        "\n",
        "  # Transform content image.\n",
        "  stylized_image = interpreter.tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"]\n",
        "  )()\n",
        "\n",
        "  return stylized_image\n",
        "\n",
        "\n",
        "def stylize(preprocessed_content_image, style_bottleneck, style_bottleneck_content, content_blending_ratio):\n",
        "  # Blend the style bottleneck of style image and content image\n",
        "  style_bottleneck_blended = content_blending_ratio * style_bottleneck_content \\\n",
        "      + (1 - content_blending_ratio) * style_bottleneck\n",
        "\n",
        "  # Stylize the content image using the style bottleneck.\n",
        "  stylized_image = run_style_transform(\n",
        "      style_bottleneck_blended, preprocessed_content_image)\n",
        "\n",
        "  print('üêå', end=\"\")\n",
        "  return stylized_image\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "  y = np.zeros(len(x))\n",
        "  for i in range(len(x)):\n",
        "    y[i] = 255 / (1 + math.exp(-x[i]))\n",
        "  return y\n",
        "\n",
        "# generate sigmoid.\n",
        "# size: the width and height in pixels\n",
        "# magnitude: the strength of the sigmoid - higher values = sharper transition.\n",
        "#            must be >0, increases above 100 don't make much difference\n",
        "# flip: if true, values change across the y-axis\n",
        "\n",
        "\n",
        "def generate_sigmoid(size, magnitude=6, flip=False):\n",
        "  m = magnitude/10\n",
        "  sigmoid_ = sigmoid(np.concatenate((np.arange(-m, m, 4*m/size),\n",
        "                                     np.arange(m, -m, -4*m/size))))\n",
        "  alpha = np.repeat(sigmoid_.reshape((len(sigmoid_), 1)), repeats=size, axis=1)\n",
        "  if flip:\n",
        "    alpha = np.swapaxes(alpha, 0, 1)\n",
        "  res = Image.fromarray(np.uint8(alpha), 'L')\n",
        "  return res\n",
        "\n",
        "\n",
        "def sew(stylized_pieces, cfg):\n",
        "  cols = cfg.cols\n",
        "  rows = cfg.rows\n",
        "  edge_size = cfg.edge_size\n",
        "  wh = cfg.content_size\n",
        "\n",
        "  im = Image.new('RGB', (\n",
        "      cols*wh-2*edge_size*(cols-1),\n",
        "      rows*wh-2*edge_size*(rows-1)\n",
        "  ))\n",
        "\n",
        "  for r in range(0, rows):\n",
        "    for c in range(0, cols):\n",
        "      squeezed = tf.squeeze(stylized_pieces[c+r*cols])\n",
        "      imaged = tf.keras.preprocessing.image.array_to_img(squeezed)\n",
        "\n",
        "      el = 0 if c == 0 else edge_size\n",
        "      er = 0 if c == cols-1 else edge_size\n",
        "      eu = 0 if r == 0 else edge_size\n",
        "      ed = 0 if r == rows-1 else edge_size\n",
        "\n",
        "      print(\"imaged size:\")\n",
        "      print(imaged.size)\n",
        "      cropped = imaged.crop((el, eu, wh-er, wh-ed))\n",
        "      print(\"cropped size:\")\n",
        "      print(cropped.size)\n",
        "\n",
        "      im.paste(cropped, (\n",
        "          c*wh - max(0, (2*c-1)*el),  # left\n",
        "          r*wh - max(0, (2*r-1)*eu)  # up\n",
        "      ))\n",
        "  return im\n",
        "\n",
        "def apply_row_joints(orig, joints, cfg):\n",
        "  cols = cfg.cols\n",
        "  rows = cfg.rows\n",
        "  edge_size = cfg.edge_size\n",
        "  wh = cfg.content_size\n",
        "  magnitude = cfg.magnitude\n",
        "\n",
        "  updated_image = orig.copy()\n",
        "  mask = generate_sigmoid(384, magnitude, True)\n",
        "  assert(rows*(cols-1) == len(joints))\n",
        "\n",
        "  for r in range(0, rows):\n",
        "    for c in range(0, cols-1):\n",
        "      squeezed = tf.squeeze(joints[c+r*(cols-1)])\n",
        "      imaged = tf.keras.preprocessing.image.array_to_img(squeezed)\n",
        "\n",
        "      el = 0 if c == 0 else edge_size\n",
        "      er = 0 if c == cols-1 else edge_size\n",
        "      eu = 0 if r == 0 else edge_size\n",
        "      ed = 0 if r == rows-1 else edge_size\n",
        "\n",
        "      cropped = imaged.crop((el, eu, wh-er, wh-ed))\n",
        "\n",
        "      # blackbox = Image.new('RGB', cropped.size, 0)\n",
        "      updated_image.paste(cropped, (\n",
        "          int((0.5+c)*wh - (0.5*edge_size + 1.5*edge_size*c)),  # left\n",
        "          r*wh - max(0, (2*r-1)*eu)  # up\n",
        "      ), mask.resize(cropped.size))\n",
        "  return updated_image\n",
        "\n",
        "\n",
        "def apply_column_joints(orig, joints, cfg):\n",
        "  cols = cfg.cols\n",
        "  rows = cfg.rows\n",
        "  edge_size = cfg.edge_size\n",
        "  wh = cfg.content_size\n",
        "  magnitude = cfg.magnitude\n",
        "\n",
        "  updated_image = orig.copy()\n",
        "  mask = generate_sigmoid(384, magnitude, False)\n",
        "  assert(((rows-1)*cols) == len(joints))\n",
        "\n",
        "  for r in range(0, rows-1):\n",
        "    for c in range(0, cols):\n",
        "      squeezed = tf.squeeze(joints[c+r*cols])\n",
        "      imaged = tf.keras.preprocessing.image.array_to_img(squeezed)\n",
        "\n",
        "      el = 0 if c == 0 else edge_size\n",
        "      er = 0 if c == cols-1 else edge_size\n",
        "      eu = 0 if r == 0 else edge_size\n",
        "      ed = 0 if r == rows-1 else edge_size\n",
        "\n",
        "      cropped = imaged.crop((el, eu, wh-er, wh-ed))\n",
        "\n",
        "      # blackbox = Image.new('RGB', cropped.size, 0)\n",
        "      updated_image.paste(cropped, (\n",
        "          c*wh - max(0, (2*c-1)*el),  # left\n",
        "          int((0.5+r)*wh - (0.5*edge_size + 1.5*edge_size*r))  # up\n",
        "      ), mask.resize(cropped.size))\n",
        "  return updated_image\n",
        "\n",
        "def get_nice_name(path):\n",
        "  # @TODO Windows needs this instead:\n",
        "  # re.search(r'.*\\\\(.*).jpg', path).group(1)\n",
        "  return re.search(r'.*/(.*).jpg', path).group(1)\n",
        "\n",
        "def get_output_filename(content_image_path, style_image_path, \n",
        "                        content_blending_ratio, edge_size, \n",
        "                        use_tiled_style_image, use_fluid_blend, magnitude,\n",
        "                        extra_id=\"\", **kwargs):\n",
        "  content_blending_ratio = float(content_blending_ratio)\n",
        "  output = \"{0}-{1}-hd-fusion-blend{2}-edge{3}{4}{5}{6}.jpg\".format(\n",
        "      get_nice_name(content_image_path),\n",
        "      get_nice_name(style_image_path),\n",
        "      int(10*content_blending_ratio),\n",
        "      edge_size,\n",
        "      \"-tiled\" if use_tiled_style_image else \"\",\n",
        "      \"-fluid{0}\".format(magnitude) if use_fluid_blend else \"\",\n",
        "      extra_id\n",
        "  )\n",
        "  return output\n",
        "\n",
        "def render(\n",
        "    drive_base,\n",
        "    content_image_path,\n",
        "    style_image_path,\n",
        "    cols=4,\n",
        "    rows=3,\n",
        "    use_tiled_style_image=False,\n",
        "    use_fluid_blend=True,\n",
        "    edge_size=4,\n",
        "    magnitude=6,\n",
        "    content_blending_ratio=0.5\n",
        "):\n",
        "  # time.sleep(1)\n",
        "  print(\"did a thing\")\n",
        "  config = dict(cols=cols, rows=rows, edge_size=edge_size, magnitude=magnitude, content_size=CONTENT_SIZE, style_size=STYLE_SIZE)\n",
        "  config = SimpleNamespace(**config)\n",
        "\n",
        "  content_image = Image.open(drive_base+content_image_path)\n",
        "  style_image = Image.open(drive_base+style_image_path)\n",
        "  \n",
        "  content_pieces = get_array_of_pieces(content_image, config)\n",
        "  if use_tiled_style_image:\n",
        "    style_pieces = get_array_of_pieces(style_image, config)\n",
        "  else:\n",
        "    style_pieces = list([style_image])\n",
        "\n",
        "  if use_fluid_blend:\n",
        "    (row_joints, col_joints) = get_intermediate_tiles(content_image, config)\n",
        "    # print(\"Row joints:\", len(row_joints), \"Col joints:\", len(col_joints))\n",
        "    content_pieces.extend(row_joints)\n",
        "    content_pieces.extend(col_joints)\n",
        " \n",
        "  tf_content_pieces = list(map(lambda x: load_content_img(np.array(x)), content_pieces))\n",
        "  preprocessed_content_pieces = list(map(lambda x: preprocessor(x, CONTENT_SIZE), content_pieces))\n",
        "  preprocessed_style_pieces = list(map(lambda x: preprocessor(x, STYLE_SIZE), style_pieces))\n",
        "\n",
        "  style_bottlenecks = list(map(lambda x: run_style_predict(x), preprocessed_style_pieces))\n",
        "  style_bottleneck_contents = list(map(lambda x: run_style_predict(preprocess_image(x, STYLE_SIZE)), tf_content_pieces))\n",
        "\n",
        "  print(\"Processing\", len(preprocessed_content_pieces), \"cells of content...\")\n",
        "\n",
        "  if use_tiled_style_image:\n",
        "    stylized_pieces = list(map(lambda x, y, z: stylize(\n",
        "        x, y, z, content_blending_ratio), preprocessed_content_pieces, style_bottlenecks, style_bottleneck_contents))\n",
        "  else:\n",
        "    stylized_pieces = list(map(lambda x, z: stylize(\n",
        "        x, style_bottlenecks[0], z, content_blending_ratio), preprocessed_content_pieces, style_bottleneck_contents))\n",
        "\n",
        "  image = sew(stylized_pieces, config)\n",
        "\n",
        "  if use_fluid_blend:\n",
        "    row_joints = stylized_pieces[(rows*cols):(rows*cols+rows*(cols-1))]\n",
        "    column_joints = stylized_pieces[(rows*cols+rows*(cols-1)):]\n",
        "    image = apply_column_joints(\n",
        "        apply_row_joints(image, row_joints, config), column_joints, config)\n",
        "\n",
        "  output_filename = \"{0}output/{1}\".format(drive_base, get_output_filename(\n",
        "    content_image_path=content_image_path,\n",
        "    style_image_path=style_image_path,\n",
        "    content_blending_ratio=content_blending_ratio,\n",
        "    edge_size=edge_size,\n",
        "    use_tiled_style_image=use_tiled_style_image,\n",
        "    use_fluid_blend=use_fluid_blend,\n",
        "    magnitude=magnitude\n",
        "  ))\n",
        "  image.save(output_filename, \"JPEG\")\n",
        "  print(\"Saved to:\", output_filename)\n",
        "  return image\n",
        "\n",
        "print(\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPf_hsc9Z25y"
      },
      "source": [
        "#@title 2. Mount Google Drive\n",
        "\n",
        "print(\"Mounting google drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbwAyRrCaRam"
      },
      "source": [
        "#@title 3. Run It\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "google_drive_base = '/content/drive/MyDrive/images/' #@param\n",
        "content_image_path = 'prettyearth/1568.jpg' #@param\n",
        "style_image_path = 'alex-grey/wp2705517-alex-grey-backgrounds-desktop.jpg' #@param\n",
        "\n",
        "#@title 3b. Divide our style image and content image into sub-blocks.\n",
        "#@markdown Here you need to set the number of rows and columns for subdivisions.\n",
        "#@markdown Each region should be 256x256 or greater, ex. 1920x1080 should be\n",
        "#@markdown _at most_ 7 columns by 4 rows. Each cell was taking me about 90 seconds to generate.\n",
        "cols = 1 #@param {type:\"slider\", min:1, max:7, step:1}\n",
        "rows = 1 #@param {type:\"slider\", min:1, max:4, step:1}\n",
        "\n",
        "#@markdown Should we calculate one style for the entire style image? Or should we \n",
        "#@markdown divide the style image into tiles just like we do with the the content?\n",
        "use_tiled_style_image = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Edge size in final image. Basically this is the amount you cut off the \n",
        "#@markdown intersecting edges of each tile. Set to 0 for no edge correction. 4 or 8\n",
        "#@markdown work real good.\n",
        "edge_size = 8 #@param {type:\"slider\", min:0, max:32, step:1}\n",
        "\n",
        "#@markdown New experimental mode of blending out the edges. Renders \"grout\" tiles\n",
        "#@markdown (you know, at the intersections of content tiles) and then mixes those\n",
        "#@markdown pixels with those of the sewn-together image. This will nearly double\n",
        "#@markdown the runtime but it's good.\n",
        "use_fluid_blend = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Sigma magnitude for fluid blend - lower # means smoother transition\n",
        "magnitude = 2 #@param {type:\"slider\", min:1, max:99}\n",
        "\n",
        "content_blending_ratio = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "#@markdown You're encouraged to play with the different values of `content_blending_ratio`.\n",
        "\n",
        "result = render(\n",
        "    google_drive_base,\n",
        "    content_image_path,\n",
        "    style_image_path,\n",
        "    cols,\n",
        "    rows,\n",
        "    use_fluid_blend,\n",
        "    edge_size,\n",
        "    magnitude,\n",
        "    content_blending_ratio\n",
        ")\n",
        "\n",
        "plt.imshow(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}